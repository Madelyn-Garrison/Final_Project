[
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Modeling",
    "section": "",
    "text": "The data set we are working with is the Diabetes Health Indicators Dataset. The data set contains 22 variables for 253,680 individuals. Those variables include whether or not a person has diabetes, demographic factors, and lifestyle characteristics. Essentially, we will be using the variables in this data set to create a model to predict whether or not a person has diabetes.\nIn this analysis, the important variables we are considering are HighBP, HighChol, BMI, AnyHealthcare, Age, GenHlth, and Income. HighBP and HighChol are binary variables for whether or not a person has high blood pressure or high cholesterol, respectively. BMI is the body mass index. AnyHealthcare is a binary variable for having any type of health care access. Age places an individual’s age into one of 13 different age groups. GenHlth has an individual rate their overall health from 1-5. Income places an individual’s income into one of 8 different groups.\nHere, we will create several different models from two different families of models to find the best one for our data set.\n\nmy_data&lt;-read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")|&gt;\n    mutate(Fruits=as.factor(ifelse(Fruits==0, \"No fruit\", \"Fruit\")),\n         Stroke=as.factor(ifelse(Stroke==0, \"No stroke\", \"Stroke\")),\n         Smoker=as.factor(ifelse(Smoker==0, \"No 100 cigs\", \"100 cigs\")),\n         HeartDiseaseorAttack=as.factor(ifelse(HeartDiseaseorAttack==0, \"No CHD/MI\", \"CHD/MI\")),\n         PhysActivity=as.factor(ifelse(PhysActivity==0, \"No physical activity past 30 days\", \"Physical activity past 30 days\")),\n         CholCheck=as.factor(ifelse(CholCheck==0, \"No cholesterol check in 5 years\", \"Cholesterol check in past 5 years\")),\n         HighChol=as.factor(ifelse(HighChol==0, \"No high cholesterol\", \"High cholesterol\")),\n         HighBP=as.factor(ifelse(HighBP==0, \"No high blood pressure\", \"High blood pressure\")),\n         Diabetes_binary=as.factor(Diabetes_binary),\n         Veggies=as.factor(ifelse(Veggies==0, \"No veggies\", \"Veggies\")),\n         HvyAlcoholConsump=as.factor(ifelse(HvyAlcoholConsump==0, \"Not heavy Drinker\", \"Heavy Drinker\")),\n         AnyHealthcare=as.factor(ifelse(AnyHealthcare==0, \"No healthcare\", \"Healthcare\")),\n         GenHlth=as.factor(ifelse(GenHlth==1, \"Excellent\", ifelse(GenHlth==2, \"Very good\", ifelse(GenHlth==3, \"Good\", ifelse(GenHlth==4, \"Fair\", \"Poor\"))))),\n         DiffWalk=as.factor(ifelse(DiffWalk==0, \"Can walk\", \"Difficult walking\")),\n         Sex=as.factor(ifelse(Sex==0, \"female\", \"male\")),\n         Age=as.factor(Age),\n         Education=as.factor(Education),\n         Income=as.factor(Income)\n         )\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe are going to split the data into a training and test set, with a 70/30 split, and we are going to create a 5 fold CV split.\n\nset.seed(15)\ndata_split &lt;- initial_split(my_data, prop = 0.70)\ndata_train &lt;- training(data_split)\ndata_test &lt;- testing(data_split)\ndata_5_fold &lt;- vfold_cv(data_train, 5)\n\nWe are going to be comparing three different recipes. The first one uses HighBP, HighChol, BMI, AnyHealthcare, and Age as predictors. The second one uses HighBP, HighChol, BMI, AnyHealthcare, Age, and GenHlth as predictors. The third one uses HighBP, HighChol, BMI, AnyHealthcare, Age, and Income as predictors.\n\ntree_rec_1 &lt;- recipe(Diabetes_binary ~ HighBP + HighChol + BMI + AnyHealthcare + Age, data = data_train) |&gt;\n  step_dummy(HighBP,HighChol,AnyHealthcare,Age)\ntree_rec_2 &lt;- recipe(Diabetes_binary ~ HighBP + HighChol + BMI + AnyHealthcare + Age + GenHlth, data = data_train) |&gt;\n  step_dummy(HighBP,HighChol,AnyHealthcare,Age,GenHlth)\ntree_rec_3 &lt;- recipe(Diabetes_binary ~ HighBP + HighChol + BMI + AnyHealthcare + Age + Income, data = data_train) |&gt;\n  step_dummy(HighBP,HighChol,AnyHealthcare,Age,Income)\n\nThe first model we will be using is a classification tree. Tree-based methods break up predictors into regions. Then each of those regions can have regions and so on. The easiest way to understand this is to visualize a flow chart. Each branch creates a new region. A classification tree, as opposed to a regression tree, predicts class membership rather than a continuous response. It will then use the most common classification in a region as the predicted classification.\nThis is the model specification for a classification tree.\n\ntree_mod &lt;- decision_tree(tree_depth = tune(),\n                          min_n = 20,\n                          cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"classification\")\n\nCreate the workflow and determine which tuning parameter is best for Recipe 1.\n\ntree_wkf_1 &lt;- workflow() |&gt;\n  add_recipe(tree_rec_1) |&gt;\n  add_model(tree_mod)\n\ntemp_1 &lt;- tree_wkf_1 |&gt; \n  tune_grid(resamples = data_5_fold, metrics = metric_set(mn_log_loss))\ntemp_1 |&gt; \n  collect_metrics()\n\n# A tibble: 10 × 8\n   cost_complexity tree_depth .metric     .estimator  mean     n std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1        1.35e- 8         13 mn_log_loss binary     0.349     5 0.00284 Prepro…\n 2        3.22e- 6          5 mn_log_loss binary     0.357     5 0.00171 Prepro…\n 3        4.14e- 4         10 mn_log_loss binary     0.357     5 0.00176 Prepro…\n 4        8.40e- 7          9 mn_log_loss binary     0.350     5 0.00175 Prepro…\n 5        4.93e- 5         11 mn_log_loss binary     0.356     5 0.00189 Prepro…\n 6        1.94e- 3          8 mn_log_loss binary     0.394     5 0.00783 Prepro…\n 7        1.77e- 9          3 mn_log_loss binary     0.403     5 0.00162 Prepro…\n 8        2.57e- 7          1 mn_log_loss binary     0.403     5 0.00162 Prepro…\n 9        8.14e- 2         15 mn_log_loss binary     0.403     5 0.00162 Prepro…\n10        1.28e-10          5 mn_log_loss binary     0.357     5 0.00171 Prepro…\n\ntemp_1 |&gt;\n  collect_metrics() |&gt;\n  arrange(mean)\n\n# A tibble: 10 × 8\n   cost_complexity tree_depth .metric     .estimator  mean     n std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1        1.35e- 8         13 mn_log_loss binary     0.349     5 0.00284 Prepro…\n 2        8.40e- 7          9 mn_log_loss binary     0.350     5 0.00175 Prepro…\n 3        4.93e- 5         11 mn_log_loss binary     0.356     5 0.00189 Prepro…\n 4        4.14e- 4         10 mn_log_loss binary     0.357     5 0.00176 Prepro…\n 5        3.22e- 6          5 mn_log_loss binary     0.357     5 0.00171 Prepro…\n 6        1.28e-10          5 mn_log_loss binary     0.357     5 0.00171 Prepro…\n 7        1.94e- 3          8 mn_log_loss binary     0.394     5 0.00783 Prepro…\n 8        1.77e- 9          3 mn_log_loss binary     0.403     5 0.00162 Prepro…\n 9        2.57e- 7          1 mn_log_loss binary     0.403     5 0.00162 Prepro…\n10        8.14e- 2         15 mn_log_loss binary     0.403     5 0.00162 Prepro…\n\ntree_best_params_1 &lt;- select_best(temp_1)\n\nWarning in select_best(temp_1): No value of `metric` was given; \"mn_log_loss\"\nwill be used.\n\n\nCreate the workflow and determine which tuning parameter is best for Recipe 2.\n\ntree_wkf_2 &lt;- workflow() |&gt;\n  add_recipe(tree_rec_2) |&gt;\n  add_model(tree_mod)\n\ntemp_2 &lt;- tree_wkf_2 |&gt; \n  tune_grid(resamples = data_5_fold, metrics = metric_set(mn_log_loss))\ntemp_2 |&gt; \n  collect_metrics()\n\n# A tibble: 10 × 8\n   cost_complexity tree_depth .metric     .estimator  mean     n std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1        3.05e-10          5 mn_log_loss binary     0.356     5 1.71e-3 Prepro…\n 2        6.10e- 2         12 mn_log_loss binary     0.403     5 1.62e-3 Prepro…\n 3        1.27e- 6          6 mn_log_loss binary     0.353     5 2.53e-3 Prepro…\n 4        6.11e- 3          3 mn_log_loss binary     0.403     5 1.62e-3 Prepro…\n 5        1.68e- 5         13 mn_log_loss binary     0.337     5 2.03e-3 Prepro…\n 6        4.39e- 9          8 mn_log_loss binary     0.338     5 1.63e-3 Prepro…\n 7        2.02e- 4         10 mn_log_loss binary     0.355     5 6.11e-4 Prepro…\n 8        3.21e- 8          2 mn_log_loss binary     0.403     5 1.62e-3 Prepro…\n 9        1.44e- 7          7 mn_log_loss binary     0.345     5 1.59e-3 Prepro…\n10        3.52e- 5         14 mn_log_loss binary     0.338     5 1.52e-3 Prepro…\n\ntemp_2 |&gt;\n  collect_metrics() |&gt;\n  arrange(mean)\n\n# A tibble: 10 × 8\n   cost_complexity tree_depth .metric     .estimator  mean     n std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1        1.68e- 5         13 mn_log_loss binary     0.337     5 2.03e-3 Prepro…\n 2        3.52e- 5         14 mn_log_loss binary     0.338     5 1.52e-3 Prepro…\n 3        4.39e- 9          8 mn_log_loss binary     0.338     5 1.63e-3 Prepro…\n 4        1.44e- 7          7 mn_log_loss binary     0.345     5 1.59e-3 Prepro…\n 5        1.27e- 6          6 mn_log_loss binary     0.353     5 2.53e-3 Prepro…\n 6        2.02e- 4         10 mn_log_loss binary     0.355     5 6.11e-4 Prepro…\n 7        3.05e-10          5 mn_log_loss binary     0.356     5 1.71e-3 Prepro…\n 8        6.10e- 2         12 mn_log_loss binary     0.403     5 1.62e-3 Prepro…\n 9        6.11e- 3          3 mn_log_loss binary     0.403     5 1.62e-3 Prepro…\n10        3.21e- 8          2 mn_log_loss binary     0.403     5 1.62e-3 Prepro…\n\ntree_best_params_2 &lt;- select_best(temp_2)\n\nWarning in select_best(temp_2): No value of `metric` was given; \"mn_log_loss\"\nwill be used.\n\n\nCreate the workflow and determine which tuning parameter is best for Recipe 3.\n\ntree_wkf_3 &lt;- workflow() |&gt;\n  add_recipe(tree_rec_3) |&gt;\n  add_model(tree_mod)\n\ntemp_3 &lt;- tree_wkf_3 |&gt; \n  tune_grid(resamples = data_5_fold, metrics = metric_set(mn_log_loss))\ntemp_3 |&gt; \n  collect_metrics()\n\n# A tibble: 10 × 8\n   cost_complexity tree_depth .metric     .estimator  mean     n std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1        6.72e- 7          4 mn_log_loss binary     0.358     5 0.00169 Prepro…\n 2        5.76e- 8         15 mn_log_loss binary     0.347     5 0.00335 Prepro…\n 3        3.15e- 8          9 mn_log_loss binary     0.350     5 0.00202 Prepro…\n 4        3.46e- 2          2 mn_log_loss binary     0.403     5 0.00162 Prepro…\n 5        1.95e- 9          3 mn_log_loss binary     0.403     5 0.00162 Prepro…\n 6        1.24e- 3          5 mn_log_loss binary     0.376     5 0.0103  Prepro…\n 7        1.17e- 4          7 mn_log_loss binary     0.357     5 0.00172 Prepro…\n 8        4.37e- 6         10 mn_log_loss binary     0.350     5 0.00213 Prepro…\n 9        2.15e- 3         12 mn_log_loss binary     0.403     5 0.00162 Prepro…\n10        1.60e-10         13 mn_log_loss binary     0.347     5 0.00296 Prepro…\n\ntemp_3 |&gt;\n  collect_metrics() |&gt;\n  arrange(mean)\n\n# A tibble: 10 × 8\n   cost_complexity tree_depth .metric     .estimator  mean     n std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1        1.60e-10         13 mn_log_loss binary     0.347     5 0.00296 Prepro…\n 2        5.76e- 8         15 mn_log_loss binary     0.347     5 0.00335 Prepro…\n 3        4.37e- 6         10 mn_log_loss binary     0.350     5 0.00213 Prepro…\n 4        3.15e- 8          9 mn_log_loss binary     0.350     5 0.00202 Prepro…\n 5        1.17e- 4          7 mn_log_loss binary     0.357     5 0.00172 Prepro…\n 6        6.72e- 7          4 mn_log_loss binary     0.358     5 0.00169 Prepro…\n 7        1.24e- 3          5 mn_log_loss binary     0.376     5 0.0103  Prepro…\n 8        3.46e- 2          2 mn_log_loss binary     0.403     5 0.00162 Prepro…\n 9        1.95e- 9          3 mn_log_loss binary     0.403     5 0.00162 Prepro…\n10        2.15e- 3         12 mn_log_loss binary     0.403     5 0.00162 Prepro…\n\ntree_best_params_3 &lt;- select_best(temp_3)\n\nWarning in select_best(temp_3): No value of `metric` was given; \"mn_log_loss\"\nwill be used.\n\n\nHere, we’ll compare the three models and choose the best, based on the lowest mn_log_loss.\n\nbest_tree_1&lt;-temp_1 |&gt;collect_metrics() |&gt;arrange(mean)\nbest_tree_2&lt;-temp_2 |&gt;collect_metrics() |&gt;arrange(mean)\nbest_tree_3&lt;-temp_3 |&gt;collect_metrics() |&gt;arrange(mean)\ncbind(c(\"Tree 1\",\"Tree 2\",\"Tree 3\"),rbind(best_tree_1[1,],best_tree_2[1,], best_tree_3[1,]))\n\n  c(\"Tree 1\", \"Tree 2\", \"Tree 3\") cost_complexity tree_depth     .metric\n1                          Tree 1    1.354271e-08         13 mn_log_loss\n2                          Tree 2    1.679638e-05         13 mn_log_loss\n3                          Tree 3    1.596664e-10         13 mn_log_loss\n  .estimator      mean n     std_err               .config\n1     binary 0.3489373 5 0.002841894 Preprocessor1_Model01\n2     binary 0.3368196 5 0.002028390 Preprocessor1_Model05\n3     binary 0.3466514 5 0.002962748 Preprocessor1_Model10\n\n\nThe second model is best for the regression tree.\nThe second model family was supposed to be random forest. The random forest creates many trees from repeated bootstrap samples and then averages those trees into the final tree. The random forest uses subsets of predictors to create each tree, keeping an overly strong predictor from skewing the model. The main differnce between random forest and classification tree is multiple trees vs just one from the entire data set.\nI attempted to use random forest, but R never finished fitting the CV folds. The computation just never finished. R on my computer has been slow lately, eventually completely failing to respond and forcing me to quit, and I’m running low on storage space. R is up to date, but one of the other things could be affecting it. Bottom line, I couldn’t create any random forest models. Instead, I used logistic regression for second family.\nLogistic regression is a model that uses a linear model predicts classification, similar to a classification tree. Coefficients of the model represent the change in log-odds of the outcome. A classification tree is non-linear and less structured than logistic regression.\nThis is the model specification for a logistic regression.\nCreate the workflow and fit the 5 fold CV to all 3 recipes.\n\nLR_spec &lt;- logistic_reg() |&gt;\n set_engine(\"glm\")\n\nLR1_wkf &lt;- workflow() |&gt;\n add_recipe(tree_rec_1) |&gt;\n add_model(LR_spec)\nLR2_wkf &lt;- workflow() |&gt;\n add_recipe(tree_rec_2) |&gt;\n add_model(LR_spec)\nLR3_wkf &lt;- workflow() |&gt;\n add_recipe(tree_rec_3) |&gt;\n add_model(LR_spec)\n\nLR1_fit &lt;- LR1_wkf |&gt;\n fit_resamples(data_5_fold, metrics = metric_set(mn_log_loss))\nLR2_fit &lt;- LR2_wkf |&gt;\n fit_resamples(data_5_fold, metrics = metric_set(mn_log_loss))\nLR3_fit &lt;- LR3_wkf |&gt;\n fit_resamples(data_5_fold, metrics = metric_set(mn_log_loss))\n\nHere, we’ll compare the three models and choose the best, based on the lowest mn_log_loss.\n\ncbind(c(\"LR 1\",\"LR 2\",\"LR 3\"),rbind(LR1_fit |&gt; collect_metrics(),\n LR2_fit |&gt; collect_metrics(),\n LR3_fit |&gt; collect_metrics()))\n\n  c(\"LR 1\", \"LR 2\", \"LR 3\")     .metric .estimator      mean n     std_err\n1                      LR 1 mn_log_loss     binary 0.3395850 5 0.001525192\n2                      LR 2 mn_log_loss     binary 0.3209221 5 0.001192447\n3                      LR 3 mn_log_loss     binary 0.3344738 5 0.001432342\n               .config\n1 Preprocessor1_Model1\n2 Preprocessor1_Model1\n3 Preprocessor1_Model1\n\n\nThe second model is best for the logistic regression.\nRefit the logistic regression model to the full data set.\n\nlast_fit(LR2_wkf, data_split, metrics = metric_set(mn_log_loss))|&gt;collect_metrics()\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mn_log_loss binary         0.323 Preprocessor1_Model1\n\ncomplete_LR&lt;-last_fit(LR2_wkf, data_split, metrics = metric_set(mn_log_loss))|&gt;collect_metrics()\n\nRefit the classification tree model to the full data set.\n\ntree_final_wkf &lt;- tree_wkf_2 |&gt;\n  finalize_workflow(tree_best_params_2)\n\ntree_final_fit &lt;- tree_final_wkf |&gt;\n  last_fit(data_split, metrics = metric_set(mn_log_loss))\ntree_final_fit\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits                 id            .metrics .notes   .predictions .workflow \n  &lt;list&gt;                 &lt;chr&gt;         &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [177576/76104]&gt; train/test s… &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\ntree_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mn_log_loss binary         0.339 Preprocessor1_Model1\n\ncomplete_tree&lt;-tree_final_fit |&gt;collect_metrics()\n\nCompare the two best models to choose the best overall model, based on lowest mn_log_loss.\n\ncbind(c(\"Tree\",\"LR\"),rbind(complete_tree, complete_LR))\n\n  c(\"Tree\", \"LR\")     .metric .estimator .estimate              .config\n1            Tree mn_log_loss     binary 0.3394528 Preprocessor1_Model1\n2              LR mn_log_loss     binary 0.3230969 Preprocessor1_Model1\n\n\nThe logistic regression model is better."
  },
  {
    "objectID": "Modeling.html#introduction",
    "href": "Modeling.html#introduction",
    "title": "Modeling",
    "section": "",
    "text": "The data set we are working with is the Diabetes Health Indicators Dataset. The data set contains 22 variables for 253,680 individuals. Those variables include whether or not a person has diabetes, demographic factors, and lifestyle characteristics. Essentially, we will be using the variables in this data set to create a model to predict whether or not a person has diabetes.\nIn this analysis, the important variables we are considering are HighBP, HighChol, BMI, AnyHealthcare, Age, GenHlth, and Income. HighBP and HighChol are binary variables for whether or not a person has high blood pressure or high cholesterol, respectively. BMI is the body mass index. AnyHealthcare is a binary variable for having any type of health care access. Age places an individual’s age into one of 13 different age groups. GenHlth has an individual rate their overall health from 1-5. Income places an individual’s income into one of 8 different groups.\nHere, we will create several different models from two different families of models to find the best one for our data set.\n\nmy_data&lt;-read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")|&gt;\n    mutate(Fruits=as.factor(ifelse(Fruits==0, \"No fruit\", \"Fruit\")),\n         Stroke=as.factor(ifelse(Stroke==0, \"No stroke\", \"Stroke\")),\n         Smoker=as.factor(ifelse(Smoker==0, \"No 100 cigs\", \"100 cigs\")),\n         HeartDiseaseorAttack=as.factor(ifelse(HeartDiseaseorAttack==0, \"No CHD/MI\", \"CHD/MI\")),\n         PhysActivity=as.factor(ifelse(PhysActivity==0, \"No physical activity past 30 days\", \"Physical activity past 30 days\")),\n         CholCheck=as.factor(ifelse(CholCheck==0, \"No cholesterol check in 5 years\", \"Cholesterol check in past 5 years\")),\n         HighChol=as.factor(ifelse(HighChol==0, \"No high cholesterol\", \"High cholesterol\")),\n         HighBP=as.factor(ifelse(HighBP==0, \"No high blood pressure\", \"High blood pressure\")),\n         Diabetes_binary=as.factor(Diabetes_binary),\n         Veggies=as.factor(ifelse(Veggies==0, \"No veggies\", \"Veggies\")),\n         HvyAlcoholConsump=as.factor(ifelse(HvyAlcoholConsump==0, \"Not heavy Drinker\", \"Heavy Drinker\")),\n         AnyHealthcare=as.factor(ifelse(AnyHealthcare==0, \"No healthcare\", \"Healthcare\")),\n         GenHlth=as.factor(ifelse(GenHlth==1, \"Excellent\", ifelse(GenHlth==2, \"Very good\", ifelse(GenHlth==3, \"Good\", ifelse(GenHlth==4, \"Fair\", \"Poor\"))))),\n         DiffWalk=as.factor(ifelse(DiffWalk==0, \"Can walk\", \"Difficult walking\")),\n         Sex=as.factor(ifelse(Sex==0, \"female\", \"male\")),\n         Age=as.factor(Age),\n         Education=as.factor(Education),\n         Income=as.factor(Income)\n         )\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe are going to split the data into a training and test set, with a 70/30 split, and we are going to create a 5 fold CV split.\n\nset.seed(15)\ndata_split &lt;- initial_split(my_data, prop = 0.70)\ndata_train &lt;- training(data_split)\ndata_test &lt;- testing(data_split)\ndata_5_fold &lt;- vfold_cv(data_train, 5)\n\nWe are going to be comparing three different recipes. The first one uses HighBP, HighChol, BMI, AnyHealthcare, and Age as predictors. The second one uses HighBP, HighChol, BMI, AnyHealthcare, Age, and GenHlth as predictors. The third one uses HighBP, HighChol, BMI, AnyHealthcare, Age, and Income as predictors.\n\ntree_rec_1 &lt;- recipe(Diabetes_binary ~ HighBP + HighChol + BMI + AnyHealthcare + Age, data = data_train) |&gt;\n  step_dummy(HighBP,HighChol,AnyHealthcare,Age)\ntree_rec_2 &lt;- recipe(Diabetes_binary ~ HighBP + HighChol + BMI + AnyHealthcare + Age + GenHlth, data = data_train) |&gt;\n  step_dummy(HighBP,HighChol,AnyHealthcare,Age,GenHlth)\ntree_rec_3 &lt;- recipe(Diabetes_binary ~ HighBP + HighChol + BMI + AnyHealthcare + Age + Income, data = data_train) |&gt;\n  step_dummy(HighBP,HighChol,AnyHealthcare,Age,Income)\n\nThe first model we will be using is a classification tree. Tree-based methods break up predictors into regions. Then each of those regions can have regions and so on. The easiest way to understand this is to visualize a flow chart. Each branch creates a new region. A classification tree, as opposed to a regression tree, predicts class membership rather than a continuous response. It will then use the most common classification in a region as the predicted classification.\nThis is the model specification for a classification tree.\n\ntree_mod &lt;- decision_tree(tree_depth = tune(),\n                          min_n = 20,\n                          cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"classification\")\n\nCreate the workflow and determine which tuning parameter is best for Recipe 1.\n\ntree_wkf_1 &lt;- workflow() |&gt;\n  add_recipe(tree_rec_1) |&gt;\n  add_model(tree_mod)\n\ntemp_1 &lt;- tree_wkf_1 |&gt; \n  tune_grid(resamples = data_5_fold, metrics = metric_set(mn_log_loss))\ntemp_1 |&gt; \n  collect_metrics()\n\n# A tibble: 10 × 8\n   cost_complexity tree_depth .metric     .estimator  mean     n std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1        1.35e- 8         13 mn_log_loss binary     0.349     5 0.00284 Prepro…\n 2        3.22e- 6          5 mn_log_loss binary     0.357     5 0.00171 Prepro…\n 3        4.14e- 4         10 mn_log_loss binary     0.357     5 0.00176 Prepro…\n 4        8.40e- 7          9 mn_log_loss binary     0.350     5 0.00175 Prepro…\n 5        4.93e- 5         11 mn_log_loss binary     0.356     5 0.00189 Prepro…\n 6        1.94e- 3          8 mn_log_loss binary     0.394     5 0.00783 Prepro…\n 7        1.77e- 9          3 mn_log_loss binary     0.403     5 0.00162 Prepro…\n 8        2.57e- 7          1 mn_log_loss binary     0.403     5 0.00162 Prepro…\n 9        8.14e- 2         15 mn_log_loss binary     0.403     5 0.00162 Prepro…\n10        1.28e-10          5 mn_log_loss binary     0.357     5 0.00171 Prepro…\n\ntemp_1 |&gt;\n  collect_metrics() |&gt;\n  arrange(mean)\n\n# A tibble: 10 × 8\n   cost_complexity tree_depth .metric     .estimator  mean     n std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1        1.35e- 8         13 mn_log_loss binary     0.349     5 0.00284 Prepro…\n 2        8.40e- 7          9 mn_log_loss binary     0.350     5 0.00175 Prepro…\n 3        4.93e- 5         11 mn_log_loss binary     0.356     5 0.00189 Prepro…\n 4        4.14e- 4         10 mn_log_loss binary     0.357     5 0.00176 Prepro…\n 5        3.22e- 6          5 mn_log_loss binary     0.357     5 0.00171 Prepro…\n 6        1.28e-10          5 mn_log_loss binary     0.357     5 0.00171 Prepro…\n 7        1.94e- 3          8 mn_log_loss binary     0.394     5 0.00783 Prepro…\n 8        1.77e- 9          3 mn_log_loss binary     0.403     5 0.00162 Prepro…\n 9        2.57e- 7          1 mn_log_loss binary     0.403     5 0.00162 Prepro…\n10        8.14e- 2         15 mn_log_loss binary     0.403     5 0.00162 Prepro…\n\ntree_best_params_1 &lt;- select_best(temp_1)\n\nWarning in select_best(temp_1): No value of `metric` was given; \"mn_log_loss\"\nwill be used.\n\n\nCreate the workflow and determine which tuning parameter is best for Recipe 2.\n\ntree_wkf_2 &lt;- workflow() |&gt;\n  add_recipe(tree_rec_2) |&gt;\n  add_model(tree_mod)\n\ntemp_2 &lt;- tree_wkf_2 |&gt; \n  tune_grid(resamples = data_5_fold, metrics = metric_set(mn_log_loss))\ntemp_2 |&gt; \n  collect_metrics()\n\n# A tibble: 10 × 8\n   cost_complexity tree_depth .metric     .estimator  mean     n std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1        3.05e-10          5 mn_log_loss binary     0.356     5 1.71e-3 Prepro…\n 2        6.10e- 2         12 mn_log_loss binary     0.403     5 1.62e-3 Prepro…\n 3        1.27e- 6          6 mn_log_loss binary     0.353     5 2.53e-3 Prepro…\n 4        6.11e- 3          3 mn_log_loss binary     0.403     5 1.62e-3 Prepro…\n 5        1.68e- 5         13 mn_log_loss binary     0.337     5 2.03e-3 Prepro…\n 6        4.39e- 9          8 mn_log_loss binary     0.338     5 1.63e-3 Prepro…\n 7        2.02e- 4         10 mn_log_loss binary     0.355     5 6.11e-4 Prepro…\n 8        3.21e- 8          2 mn_log_loss binary     0.403     5 1.62e-3 Prepro…\n 9        1.44e- 7          7 mn_log_loss binary     0.345     5 1.59e-3 Prepro…\n10        3.52e- 5         14 mn_log_loss binary     0.338     5 1.52e-3 Prepro…\n\ntemp_2 |&gt;\n  collect_metrics() |&gt;\n  arrange(mean)\n\n# A tibble: 10 × 8\n   cost_complexity tree_depth .metric     .estimator  mean     n std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1        1.68e- 5         13 mn_log_loss binary     0.337     5 2.03e-3 Prepro…\n 2        3.52e- 5         14 mn_log_loss binary     0.338     5 1.52e-3 Prepro…\n 3        4.39e- 9          8 mn_log_loss binary     0.338     5 1.63e-3 Prepro…\n 4        1.44e- 7          7 mn_log_loss binary     0.345     5 1.59e-3 Prepro…\n 5        1.27e- 6          6 mn_log_loss binary     0.353     5 2.53e-3 Prepro…\n 6        2.02e- 4         10 mn_log_loss binary     0.355     5 6.11e-4 Prepro…\n 7        3.05e-10          5 mn_log_loss binary     0.356     5 1.71e-3 Prepro…\n 8        6.10e- 2         12 mn_log_loss binary     0.403     5 1.62e-3 Prepro…\n 9        6.11e- 3          3 mn_log_loss binary     0.403     5 1.62e-3 Prepro…\n10        3.21e- 8          2 mn_log_loss binary     0.403     5 1.62e-3 Prepro…\n\ntree_best_params_2 &lt;- select_best(temp_2)\n\nWarning in select_best(temp_2): No value of `metric` was given; \"mn_log_loss\"\nwill be used.\n\n\nCreate the workflow and determine which tuning parameter is best for Recipe 3.\n\ntree_wkf_3 &lt;- workflow() |&gt;\n  add_recipe(tree_rec_3) |&gt;\n  add_model(tree_mod)\n\ntemp_3 &lt;- tree_wkf_3 |&gt; \n  tune_grid(resamples = data_5_fold, metrics = metric_set(mn_log_loss))\ntemp_3 |&gt; \n  collect_metrics()\n\n# A tibble: 10 × 8\n   cost_complexity tree_depth .metric     .estimator  mean     n std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1        6.72e- 7          4 mn_log_loss binary     0.358     5 0.00169 Prepro…\n 2        5.76e- 8         15 mn_log_loss binary     0.347     5 0.00335 Prepro…\n 3        3.15e- 8          9 mn_log_loss binary     0.350     5 0.00202 Prepro…\n 4        3.46e- 2          2 mn_log_loss binary     0.403     5 0.00162 Prepro…\n 5        1.95e- 9          3 mn_log_loss binary     0.403     5 0.00162 Prepro…\n 6        1.24e- 3          5 mn_log_loss binary     0.376     5 0.0103  Prepro…\n 7        1.17e- 4          7 mn_log_loss binary     0.357     5 0.00172 Prepro…\n 8        4.37e- 6         10 mn_log_loss binary     0.350     5 0.00213 Prepro…\n 9        2.15e- 3         12 mn_log_loss binary     0.403     5 0.00162 Prepro…\n10        1.60e-10         13 mn_log_loss binary     0.347     5 0.00296 Prepro…\n\ntemp_3 |&gt;\n  collect_metrics() |&gt;\n  arrange(mean)\n\n# A tibble: 10 × 8\n   cost_complexity tree_depth .metric     .estimator  mean     n std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1        1.60e-10         13 mn_log_loss binary     0.347     5 0.00296 Prepro…\n 2        5.76e- 8         15 mn_log_loss binary     0.347     5 0.00335 Prepro…\n 3        4.37e- 6         10 mn_log_loss binary     0.350     5 0.00213 Prepro…\n 4        3.15e- 8          9 mn_log_loss binary     0.350     5 0.00202 Prepro…\n 5        1.17e- 4          7 mn_log_loss binary     0.357     5 0.00172 Prepro…\n 6        6.72e- 7          4 mn_log_loss binary     0.358     5 0.00169 Prepro…\n 7        1.24e- 3          5 mn_log_loss binary     0.376     5 0.0103  Prepro…\n 8        3.46e- 2          2 mn_log_loss binary     0.403     5 0.00162 Prepro…\n 9        1.95e- 9          3 mn_log_loss binary     0.403     5 0.00162 Prepro…\n10        2.15e- 3         12 mn_log_loss binary     0.403     5 0.00162 Prepro…\n\ntree_best_params_3 &lt;- select_best(temp_3)\n\nWarning in select_best(temp_3): No value of `metric` was given; \"mn_log_loss\"\nwill be used.\n\n\nHere, we’ll compare the three models and choose the best, based on the lowest mn_log_loss.\n\nbest_tree_1&lt;-temp_1 |&gt;collect_metrics() |&gt;arrange(mean)\nbest_tree_2&lt;-temp_2 |&gt;collect_metrics() |&gt;arrange(mean)\nbest_tree_3&lt;-temp_3 |&gt;collect_metrics() |&gt;arrange(mean)\ncbind(c(\"Tree 1\",\"Tree 2\",\"Tree 3\"),rbind(best_tree_1[1,],best_tree_2[1,], best_tree_3[1,]))\n\n  c(\"Tree 1\", \"Tree 2\", \"Tree 3\") cost_complexity tree_depth     .metric\n1                          Tree 1    1.354271e-08         13 mn_log_loss\n2                          Tree 2    1.679638e-05         13 mn_log_loss\n3                          Tree 3    1.596664e-10         13 mn_log_loss\n  .estimator      mean n     std_err               .config\n1     binary 0.3489373 5 0.002841894 Preprocessor1_Model01\n2     binary 0.3368196 5 0.002028390 Preprocessor1_Model05\n3     binary 0.3466514 5 0.002962748 Preprocessor1_Model10\n\n\nThe second model is best for the regression tree.\nThe second model family was supposed to be random forest. The random forest creates many trees from repeated bootstrap samples and then averages those trees into the final tree. The random forest uses subsets of predictors to create each tree, keeping an overly strong predictor from skewing the model. The main differnce between random forest and classification tree is multiple trees vs just one from the entire data set.\nI attempted to use random forest, but R never finished fitting the CV folds. The computation just never finished. R on my computer has been slow lately, eventually completely failing to respond and forcing me to quit, and I’m running low on storage space. R is up to date, but one of the other things could be affecting it. Bottom line, I couldn’t create any random forest models. Instead, I used logistic regression for second family.\nLogistic regression is a model that uses a linear model predicts classification, similar to a classification tree. Coefficients of the model represent the change in log-odds of the outcome. A classification tree is non-linear and less structured than logistic regression.\nThis is the model specification for a logistic regression.\nCreate the workflow and fit the 5 fold CV to all 3 recipes.\n\nLR_spec &lt;- logistic_reg() |&gt;\n set_engine(\"glm\")\n\nLR1_wkf &lt;- workflow() |&gt;\n add_recipe(tree_rec_1) |&gt;\n add_model(LR_spec)\nLR2_wkf &lt;- workflow() |&gt;\n add_recipe(tree_rec_2) |&gt;\n add_model(LR_spec)\nLR3_wkf &lt;- workflow() |&gt;\n add_recipe(tree_rec_3) |&gt;\n add_model(LR_spec)\n\nLR1_fit &lt;- LR1_wkf |&gt;\n fit_resamples(data_5_fold, metrics = metric_set(mn_log_loss))\nLR2_fit &lt;- LR2_wkf |&gt;\n fit_resamples(data_5_fold, metrics = metric_set(mn_log_loss))\nLR3_fit &lt;- LR3_wkf |&gt;\n fit_resamples(data_5_fold, metrics = metric_set(mn_log_loss))\n\nHere, we’ll compare the three models and choose the best, based on the lowest mn_log_loss.\n\ncbind(c(\"LR 1\",\"LR 2\",\"LR 3\"),rbind(LR1_fit |&gt; collect_metrics(),\n LR2_fit |&gt; collect_metrics(),\n LR3_fit |&gt; collect_metrics()))\n\n  c(\"LR 1\", \"LR 2\", \"LR 3\")     .metric .estimator      mean n     std_err\n1                      LR 1 mn_log_loss     binary 0.3395850 5 0.001525192\n2                      LR 2 mn_log_loss     binary 0.3209221 5 0.001192447\n3                      LR 3 mn_log_loss     binary 0.3344738 5 0.001432342\n               .config\n1 Preprocessor1_Model1\n2 Preprocessor1_Model1\n3 Preprocessor1_Model1\n\n\nThe second model is best for the logistic regression.\nRefit the logistic regression model to the full data set.\n\nlast_fit(LR2_wkf, data_split, metrics = metric_set(mn_log_loss))|&gt;collect_metrics()\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mn_log_loss binary         0.323 Preprocessor1_Model1\n\ncomplete_LR&lt;-last_fit(LR2_wkf, data_split, metrics = metric_set(mn_log_loss))|&gt;collect_metrics()\n\nRefit the classification tree model to the full data set.\n\ntree_final_wkf &lt;- tree_wkf_2 |&gt;\n  finalize_workflow(tree_best_params_2)\n\ntree_final_fit &lt;- tree_final_wkf |&gt;\n  last_fit(data_split, metrics = metric_set(mn_log_loss))\ntree_final_fit\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits                 id            .metrics .notes   .predictions .workflow \n  &lt;list&gt;                 &lt;chr&gt;         &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [177576/76104]&gt; train/test s… &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\ntree_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mn_log_loss binary         0.339 Preprocessor1_Model1\n\ncomplete_tree&lt;-tree_final_fit |&gt;collect_metrics()\n\nCompare the two best models to choose the best overall model, based on lowest mn_log_loss.\n\ncbind(c(\"Tree\",\"LR\"),rbind(complete_tree, complete_LR))\n\n  c(\"Tree\", \"LR\")     .metric .estimator .estimate              .config\n1            Tree mn_log_loss     binary 0.3394528 Preprocessor1_Model1\n2              LR mn_log_loss     binary 0.3230969 Preprocessor1_Model1\n\n\nThe logistic regression model is better."
  },
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "EDA",
    "section": "",
    "text": "The data set we are working with is the Diabetes Health Indicators Dataset. The data set contains 22 variables for 253,680 individuals. Those variables include whether or not a person has diabetes, demographic factors, and lifestyle characteristics. Essentially, we will be using the variables in this data set to create a model to predict whether or not a person has diabetes.\nIn this analysis, the important variables we are considering are HighBP, HighChol, BMI, AnyHealthcare, Age, GenHlth, and Income. HighBP and HighChol are binary variables for whether or not a person has high blood pressure or high cholesterol, respectively. BMI is the body mass index. AnyHealthcare is a binary variable for having any type of health care access. Age places an individual’s age into one of 13 different age groups. GenHlth has an individual rate their overall health from 1-5. Income places an individual’s income into one of 8 different groups.\nHere we will perform some exploratory data analysis, do gain a better understanding of our data."
  },
  {
    "objectID": "EDA.html#introduction",
    "href": "EDA.html#introduction",
    "title": "EDA",
    "section": "",
    "text": "The data set we are working with is the Diabetes Health Indicators Dataset. The data set contains 22 variables for 253,680 individuals. Those variables include whether or not a person has diabetes, demographic factors, and lifestyle characteristics. Essentially, we will be using the variables in this data set to create a model to predict whether or not a person has diabetes.\nIn this analysis, the important variables we are considering are HighBP, HighChol, BMI, AnyHealthcare, Age, GenHlth, and Income. HighBP and HighChol are binary variables for whether or not a person has high blood pressure or high cholesterol, respectively. BMI is the body mass index. AnyHealthcare is a binary variable for having any type of health care access. Age places an individual’s age into one of 13 different age groups. GenHlth has an individual rate their overall health from 1-5. Income places an individual’s income into one of 8 different groups.\nHere we will perform some exploratory data analysis, do gain a better understanding of our data."
  },
  {
    "objectID": "EDA.html#eda",
    "href": "EDA.html#eda",
    "title": "EDA",
    "section": "EDA",
    "text": "EDA\n\nmy_diabetes_data&lt;-read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nFirst, we check for missing data. There is none.\n\ncolSums(is.na(my_diabetes_data))\n\n     Diabetes_binary               HighBP             HighChol \n                   0                    0                    0 \n           CholCheck                  BMI               Smoker \n                   0                    0                    0 \n              Stroke HeartDiseaseorAttack         PhysActivity \n                   0                    0                    0 \n              Fruits              Veggies    HvyAlcoholConsump \n                   0                    0                    0 \n       AnyHealthcare          NoDocbcCost              GenHlth \n                   0                    0                    0 \n            MentHlth             PhysHlth             DiffWalk \n                   0                    0                    0 \n                 Sex                  Age            Education \n                   0                    0                    0 \n              Income \n                   0 \n\nunique(my_diabetes_data$Diabetes_binary)\n\n[1] 0 1\n\n\nWe’ll convert many of our variables into factors with meaningful names.\n\nmy__data&lt;-my_diabetes_data|&gt;\n  mutate(Fruits=as.factor(ifelse(Fruits==0, \"No fruit\", \"Fruit\")),\n         Stroke=as.factor(ifelse(Stroke==0, \"No stroke\", \"Stroke\")),\n         Smoker=as.factor(ifelse(Smoker==0, \"No 100 cigs\", \"100 cigs\")),\n         HeartDiseaseorAttack=as.factor(ifelse(HeartDiseaseorAttack==0, \"No CHD/MI\", \"CHD/MI\")),\n         PhysActivity=as.factor(ifelse(PhysActivity==0, \"No physical activity past 30 days\", \"Physical activity past 30 days\")),\n         CholCheck=as.factor(ifelse(CholCheck==0, \"No cholesterol check in 5 years\", \"Cholesterol check in past 5 years\")),\n         HighChol=as.factor(ifelse(HighChol==0, \"No high cholesterol\", \"High cholesterol\")),\n         HighBP=as.factor(ifelse(HighBP==0, \"No high blood pressure\", \"High blood pressure\")),\n         Diabetes_binary=ifelse(Diabetes_binary==0, \"No diabetes\", \"Diabetes\"),\n         Veggies=as.factor(ifelse(Veggies==0, \"No veggies\", \"Veggies\")),\n         HvyAlcoholConsump=as.factor(ifelse(HvyAlcoholConsump==0, \"Not heavy Drinker\", \"Heavy Drinker\")),\n         AnyHealthcare=as.factor(ifelse(AnyHealthcare==0, \"No healthcare\", \"Healthcare\")),\n         NoDocbcCost=as.factor(ifelse(NoDocbcCost==0, \"No doctor\", \"Doctor\")),\n         GenHlth=as.factor(ifelse(GenHlth==1, \"Excellent\", ifelse(GenHlth==2, \"Very good\", ifelse(GenHlth==3, \"Good\", ifelse(GenHlth==4, \"Fair\", \"Poor\"))))),\n         DiffWalk=as.factor(ifelse(DiffWalk==0, \"Can walk\", \"Difficult walking\")),\n         Sex=as.factor(ifelse(Sex==0, \"female\", \"male\")),\n         Age=as.factor(Age),\n         Education=as.factor(Education),\n         Income=as.factor(Income)\n         )\n\nNext, I’m going to create a correlation matrix, to understand the realtionship between the variables. This matrix helps to decide which variables will be important for our model.\n\ncorr &lt;- cor(my_diabetes_data)\nggcorrplot(corr,\n           type = \"lower\")\n\n\n\n\n\n\n\n\nCreate bar plots or histograms for the variables we care about.\n\nggplot(my__data, aes(x=BMI)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nggplot(my__data, aes(x=HighBP)) + \n  geom_bar()\n\n\n\n\n\n\n\nggplot(my__data, aes(x=HighChol)) + \n  geom_bar()\n\n\n\n\n\n\n\nggplot(my__data, aes(x=AnyHealthcare)) + \n  geom_bar()\n\n\n\n\n\n\n\nggplot(my__data, aes(x=Age)) + \n  geom_bar()\n\n\n\n\n\n\n\nggplot(my__data, aes(x=GenHlth)) + \n  geom_bar()\n\n\n\n\n\n\n\nggplot(my__data, aes(x=Income)) + \n  geom_bar()\n\n\n\n\n\n\n\nggplot(my__data, aes(x=Diabetes_binary)) + \n  geom_bar() \n\n\n\n\n\n\n\n\nThis bar plot and table show the relationship between Diabetes and General Health, putting emphasis on the presence of diabetes in the different health categories.\n\ntable(my__data$Diabetes_binary, my__data$GenHlth)\n\n             \n              Excellent  Fair  Good  Poor Very good\n  Diabetes         1140  9790 13457  4578      6381\n  No diabetes     44159 21780 62189  7503     82703\n\nggplot(my__data, aes(x=Diabetes_binary))+\n  geom_bar()+\n  facet_wrap(~GenHlth)\n\n\n\n\n\n\n\n\nOur correlation matrix shows a relationship between General Health and Income. We’ll create an additional bar plot that shows that the distribution of General Health is not consistent across income levels.\n\ntable(my__data$GenHlth, my__data$Income)\n\n           \n                1     2     3     4     5     6     7     8\n  Excellent   796   785  1431  2045  3089  5335  7828 23990\n  Fair       2989  3382  3961  4136  4255  4350  3846  4651\n  Good       2802  3675  5431  7142  9093 12546 13306 21651\n  Poor       1794  1986  1841  1665  1424  1315  1012  1044\n  Very good  1430  1955  3330  5147  8022 12924 17227 39049\n\nggplot(my__data, aes(x=GenHlth))+\n  geom_bar()+\n  facet_wrap(~Income)\n\n\n\n\n\n\n\n\nJust income and diabetes.\n\ntable(my__data$Diabetes_binary, my__data$Income)\n\n             \n                  1     2     3     4     5     6     7     8\n  Diabetes     2383  3086  3568  4054  4504  5291  5265  7195\n  No diabetes  7428  8697 12426 16081 21379 31179 37954 83190\n\nggplot(my__data, aes(x=Diabetes_binary))+\n  geom_bar()+\n  facet_wrap(~Income)\n\n\n\n\n\n\n\n\nDistribution of diabetes based on blood pressure level.\n\ntable(my__data$Diabetes_binary, my__data$HighBP)\n\n             \n              High blood pressure No high blood pressure\n  Diabetes                  26604                   8742\n  No diabetes               82225                 136109\n\nggplot(my__data, aes(x=Diabetes_binary))+\n  geom_bar()+\n  facet_wrap(~HighBP)\n\n\n\n\n\n\n\n\nDistribution of diabetes based on cholesterol level.\n\ntable(my__data$Diabetes_binary, my__data$HighChol)\n\n             \n              High cholesterol No high cholesterol\n  Diabetes               23686               11660\n  No diabetes            83905              134429\n\nggplot(my__data, aes(x=Diabetes_binary))+\n  geom_bar()+\n  facet_wrap(~HighChol)\n\n\n\n\n\n\n\n\nDistribution of diabetes based on healthcare access.\n\ntable(my__data$Diabetes_binary, my__data$AnyHealthcare)\n\n             \n              Healthcare No healthcare\n  Diabetes         33924          1422\n  No diabetes     207339         10995\n\nggplot(my__data, aes(x=Diabetes_binary))+\n  geom_bar()+\n  facet_wrap(~AnyHealthcare)\n\n\n\n\n\n\n\n\nDistribution of diabetes based on age.\n\ntable(my__data$Diabetes_binary, my__data$Age)\n\n             \n                  1     2     3     4     5     6     7     8     9    10    11\n  Diabetes       78   140   314   626  1051  1742  3088  4263  5733  6558  5141\n  No diabetes  5622  7458 10809 13197 15106 18077 23226 26569 27511 25636 18392\n             \n                 12    13\n  Diabetes     3403  3209\n  No diabetes 12577 14154\n\nggplot(my__data, aes(x=Diabetes_binary))+\n  geom_bar()+\n  facet_wrap(~Age)\n\n\n\n\n\n\n\n\nSummary statistics and histogram of BMI for each diabetes classification.\n\nmy__data |&gt;\n  summarize(mean=mean(BMI), sd=sd(BMI), range = (max(BMI)-min(BMI)))\n\n# A tibble: 1 × 3\n   mean    sd range\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  28.4  6.61    86\n\nmy__data |&gt;\n  group_by(Diabetes_binary)|&gt;\n  summarize(mean=mean(BMI), sd=sd(BMI), range = (max(BMI)-min(BMI)))\n\n# A tibble: 2 × 4\n  Diabetes_binary  mean    sd range\n  &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Diabetes         31.9  7.36    85\n2 No diabetes      27.8  6.29    86\n\nggplot(my__data, aes(x=BMI))+\n  geom_histogram()+\n  facet_wrap(~Diabetes_binary)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  }
]